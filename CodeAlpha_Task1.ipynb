{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"nbformat\": 4,\n",
        "  \"nbformat_minor\": 0,\n",
        "  \"metadata\": {\n",
        "    \"colab\": {\n",
        "      \"provenance\": []\n",
        "    },\n",
        "    \"kernelspec\": {\n",
        "      \"name\": \"python3\",\n",
        "      \"display_name\": \"Python 3\"\n",
        "    },\n",
        "    \"language_info\": {\n",
        "      \"name\": \"python\"\n",
        "    }\n",
        "  },\n",
        "  \"cells\": [\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"execution_count\": 4,\n",
        "      \"metadata\": {\n",
        "        \"id\": \"gi9MbYEGm-Rv\"\n",
        "      },\n",
        "      \"outputs\": [],\n",
        "      \"source\": [\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"import requests\\n\",\n",
        "        \"from bs4 import BeautifulSoup\\n\",\n",
        "        \"import time\\n\",\n",
        "        \"import csv\\n\",\n",
        "        \"import random\\n\",\n",
        "        \"from urllib.parse import urljoin\\n\",\n",
        "        \"import os\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"BASE_URL = \\\"http://books.toscrape.com/\\\"\\n\",\n",
        "        \"START_URL = BASE_URL\\n\",\n",
        "        \"OUTPUT_DIR = \\\"task1_outputs\\\"\\n\",\n",
        "        \"OUTPUT_CSV = os.path.join(OUTPUT_DIR, \\\"books.csv\\\")\\n\",\n",
        "        \"USER_AGENT = \\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \\\"\\\\\\n\",\n",
        "        \"             \\\"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\\\"\\n\",\n",
        "        \"REQUEST_HEADERS = {\\\"User-Agent\\\": USER_AGENT}\\n\",\n",
        "        \"DELAY_MIN = 1.0\\n\",\n",
        "        \"DELAY_MAX = 2.5\\n\",\n",
        "        \"MAX_RETRIES = 3\\n\",\n",
        "        \"TIMEOUT = 10\\n\",\n",
        "        \"os.makedirs(OUTPUT_DIR, exist_ok=True)\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"id\": \"D_VwcTyNnmpT\"\n",
        "      },\n",
        "      \"execution_count\": 8,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"def safe_get(url, session=None, retries=MAX_RETRIES):\\n\",\n",
        "        \"    s = session or requests.Session()\\n\",\n",
        "        \"    for attempt in range(1, retries+1):\\n\",\n",
        "        \"        try:\\n\",\n",
        "        \"            resp = s.get(url, headers=REQUEST_HEADERS, timeout=TIMEOUT)\\n\",\n",
        "        \"            resp.raise_for_status()\\n\",\n",
        "        \"            return resp\\n\",\n",
        "        \"        except Exception as e:\\n\",\n",
        "        \"            print(f\\\"[warn] request failed ({attempt}/{retries}) for {url}: {e}\\\")\\n\",\n",
        "        \"            if attempt == retries:\\n\",\n",
        "        \"                raise\\n\",\n",
        "        \"            time.sleep(1.5 * attempt)\\n\",\n",
        "        \"    return None\\n\",\n",
        "        \"\\n\",\n",
        "        \"def random_delay():\\n\",\n",
        "        \"    time.sleep(random.uniform(DELAY_MIN, DELAY_MAX))\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"id\": \"NBl9yOLrnstJ\"\n",
        "      },\n",
        "      \"execution_count\": 9,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"def parse_book_card(card):\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"    title_tag = card.select_one(\\\"h3 a\\\")\\n\",\n",
        "        \"    title = title_tag[\\\"title\\\"].strip() if title_tag and title_tag.has_attr(\\\"title\\\") else title_tag.get_text(strip=True)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    rel_url = title_tag[\\\"href\\\"] if title_tag and title_tag.has_attr(\\\"href\\\") else \\\"\\\"\\n\",\n",
        "        \"    product_url = urljoin(BASE_URL, rel_url)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    price_tag = card.select_one(\\\".price_color\\\")\\n\",\n",
        "        \"    price = price_tag.get_text(strip=True) if price_tag else \\\"\\\"\\n\",\n",
        "        \"\\n\",\n",
        "        \"    availability = card.select_one(\\\".availability\\\").get_text(strip=True) if card.select_one(\\\".availability\\\") else \\\"\\\"\\n\",\n",
        "        \"\\n\",\n",
        "        \"    rating_class = card.select_one(\\\"p.star-rating\\\")\\n\",\n",
        "        \"    rating = \\\"\\\"\\n\",\n",
        "        \"    if rating_class and rating_class.has_attr(\\\"class\\\"):\\n\",\n",
        "        \"        classes = rating_class[\\\"class\\\"]\\n\",\n",
        "        \"\\n\",\n",
        "        \"        rating = [c for c in classes if c != \\\"star-rating\\\"][0] if len(classes) > 1 else \\\"\\\"\\n\",\n",
        "        \"    return {\\n\",\n",
        "        \"        \\\"title\\\": title,\\n\",\n",
        "        \"        \\\"product_url\\\": product_url,\\n\",\n",
        "        \"        \\\"price\\\": price,\\n\",\n",
        "        \"        \\\"availability\\\": availability,\\n\",\n",
        "        \"        \\\"rating\\\": rating\\n\",\n",
        "        \"    }\\n\",\n",
        "        \"\\n\",\n",
        "        \"def scrape_books(start_url=START_URL, max_pages=None):\\n\",\n",
        "        \"    results = []\\n\",\n",
        "        \"    next_page_url = start_url\\n\",\n",
        "        \"    page_count = 0\\n\",\n",
        "        \"    session = requests.Session()\\n\",\n",
        "        \"\\n\",\n",
        "        \"    while next_page_url:\\n\",\n",
        "        \"        page_count += 1\\n\",\n",
        "        \"        print(f\\\"[info] Fetching page {page_count}: {next_page_url}\\\")\\n\",\n",
        "        \"        resp = safe_get(next_page_url, session=session)\\n\",\n",
        "        \"        soup = BeautifulSoup(resp.text, \\\"html.parser\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"        cards = soup.select(\\\"article.product_pod\\\")\\n\",\n",
        "        \"        for c in cards:\\n\",\n",
        "        \"            data = parse_book_card(c)\\n\",\n",
        "        \"            results.append(data)\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"        next_btn = soup.select_one(\\\"li.next a\\\")\\n\",\n",
        "        \"        if next_btn and next_btn.has_attr(\\\"href\\\"):\\n\",\n",
        "        \"            rel_next = next_btn[\\\"href\\\"]\\n\",\n",
        "        \"            next_page_url = urljoin(next_page_url, rel_next)\\n\",\n",
        "        \"        else:\\n\",\n",
        "        \"            next_page_url = None\\n\",\n",
        "        \"\\n\",\n",
        "        \"        page_count += 0\\n\",\n",
        "        \"        random_delay()\\n\",\n",
        "        \"\\n\",\n",
        "        \"        if max_pages and page_count >= max_pages:\\n\",\n",
        "        \"            print(\\\"[info] Reached max_pages limit.\\\")\\n\",\n",
        "        \"            break\\n\",\n",
        "        \"\\n\",\n",
        "        \"    return results\\n\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"id\": \"U13uO6gxn6wR\"\n",
        "      },\n",
        "      \"execution_count\": 10,\n",
        "      \"outputs\": []\n",
        "    },\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"if __name__ == \\\"__main__\\\":\\n\",\n",
        "        \"    print(\\\"[start] scraping started\\\")\\n\",\n",
        "        \"    try:\\n\",\n",
        "        \"        data = scrape_books(max_pages=None)  # set an int to limit pages\\n\",\n",
        "        \"        # Save to CSV\\n\",\n",
        "        \"        fieldnames = [\\\"title\\\", \\\"product_url\\\", \\\"price\\\", \\\"availability\\\", \\\"rating\\\"]\\n\",\n",
        "        \"        with open(OUTPUT_CSV, \\\"w\\\", newline=\\\"\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
        "        \"            writer = csv.DictWriter(f, fieldnames=fieldnames)\\n\",\n",
        "        \"            writer.writeheader()\\n\",\n",
        "        \"            for row in data:\\n\",\n",
        "        \"                writer.writerow(row)\\n\",\n",
        "        \"        print(f\\\"[done] saved {len(data)} rows to {OUTPUT_CSV}\\\")\\n\",\n",
        "        \"    except Exception as e:\\n\",\n",
        "        \"        print(f\\\"[error] scraping aborted: {e}\\\")\\n\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"colab\": {\n",
        "          \"base_uri\": \"https://localhost:8080/\"\n",
        "        },\n",
        "        \"id\": \"zBmf1tZYoZFb\",\n",
        "        \"outputId\": \"043a4def-8fd5-4568-8346-06594494b434\"\n",
        "      },\n",
        "      \"execution_count\": 11,\n",
        "      \"outputs\": [\n",
        "        {\n",
        "          \"output_type\": \"stream\",\n",
        "          \"name\": \"stdout\",\n",
        "          \"text\": [\n",
        "            \"[start] scraping started\\n\",\n",
        "            \"[info] Fetching page 1: http://books.toscrape.com/\\n\",\n",
        "            \"[info] Fetching page 2: http://books.toscrape.com/catalogue/page-2.html\\n\",\n",
        "            \"[info] Fetching page 3: http://books.toscrape.com/catalogue/page-3.html\\n\",\n",
        "            \"[info] Fetching page 4: http://books.toscrape.com/catalogue/page-4.html\\n\",\n",
        "            \"[info] Fetching page 5: http://books.toscrape.com/catalogue/page-5.html\\n\",\n",
        "            \"[info] Fetching page 6: http://books.toscrape.com/catalogue/page-6.html\\n\",\n",
        "            \"[info] Fetching page 7: http://books.toscrape.com/catalogue/page-7.html\\n\",\n",
        "            \"[info] Fetching page 8: http://books.toscrape.com/catalogue/page-8.html\\n\",\n",
        "            \"[info] Fetching page 9: http://books.toscrape.com/catalogue/page-9.html\\n\",\n",
        "            \"[info] Fetching page 10: http://books.toscrape.com/catalogue/page-10.html\\n\",\n",
        "            \"[info] Fetching page 11: http://books.toscrape.com/catalogue/page-11.html\\n\",\n",
        "            \"[info] Fetching page 12: http://books.toscrape.com/catalogue/page-12.html\\n\",\n",
        "            \"[info] Fetching page 13: http://books.toscrape.com/catalogue/page-13.html\\n\",\n",
        "            \"[info] Fetching page 14: http://books.toscrape.com/catalogue/page-14.html\\n\",\n",
        "            \"[info] Fetching page 15: http://books.toscrape.com/catalogue/page-15.html\\n\",\n",
        "            \"[info] Fetching page 16: http://books.toscrape.com/catalogue/page-16.html\\n\",\n",
        "            \"[info] Fetching page 17: http://books.toscrape.com/catalogue/page-17.html\\n\",\n",
        "            \"[info] Fetching page 18: http://books.toscrape.com/catalogue/page-18.html\\n\",\n",
        "            \"[info] Fetching page 19: http://books.toscrape.com/catalogue/page-19.html\\n\",\n",
        "            \"[info] Fetching page 20: http://books.toscrape.com/catalogue/page-20.html\\n\",\n",
        "            \"[info] Fetching page 21: http://books.toscrape.com/catalogue/page-21.html\\n\",\n",
        "            \"[info] Fetching page 22: http://books.toscrape.com/catalogue/page-22.html\\n\",\n",
        "            \"[info] Fetching page 23: http://books.toscrape.com/catalogue/page-23.html\\n\",\n",
        "            \"[info] Fetching page 24: http://books.toscrape.com/catalogue/page-24.html\\n\",\n",
        "            \"[info] Fetching page 25: http://books.toscrape.com/catalogue/page-25.html\\n\",\n",
        "            \"[info] Fetching page 26: http://books.toscrape.com/catalogue/page-26.html\\n\",\n",
        "            \"[info] Fetching page 27: http://books.toscrape.com/catalogue/page-27.html\\n\",\n",
        "            \"[info] Fetching page 28: http://books.toscrape.com/catalogue/page-28.html\\n\",\n",
        "            \"[info] Fetching page 29: http://books.toscrape.com/catalogue/page-29.html\\n\",\n",
        "            \"[info] Fetching page 30: http://books.toscrape.com/catalogue/page-30.html\\n\",\n",
        "            \"[info] Fetching page 31: http://books.toscrape.com/catalogue/page-31.html\\n\",\n",
        "            \"[info] Fetching page 32: http://books.toscrape.com/catalogue/page-32.html\\n\",\n",
        "            \"[info] Fetching page 33: http://books.toscrape.com/catalogue/page-33.html\\n\",\n",
        "            \"[info] Fetching page 34: http://books.toscrape.com/catalogue/page-34.html\\n\",\n",
        "            \"[info] Fetching page 35: http://books.toscrape.com/catalogue/page-35.html\\n\",\n",
        "            \"[info] Fetching page 36: http://books.toscrape.com/catalogue/page-36.html\\n\",\n",
        "            \"[info] Fetching page 37: http://books.toscrape.com/catalogue/page-37.html\\n\",\n",
        "            \"[info] Fetching page 38: http://books.toscrape.com/catalogue/page-38.html\\n\",\n",
        "            \"[info] Fetching page 39: http://books.toscrape.com/catalogue/page-39.html\\n\",\n",
        "            \"[info] Fetching page 40: http://books.toscrape.com/catalogue/page-40.html\\n\",\n",
        "            \"[info] Fetching page 41: http://books.toscrape.com/catalogue/page-41.html\\n\",\n",
        "            \"[info] Fetching page 42: http://books.toscrape.com/catalogue/page-42.html\\n\",\n",
        "            \"[info] Fetching page 43: http://books.toscrape.com/catalogue/page-43.html\\n\",\n",
        "            \"[info] Fetching page 44: http://books.toscrape.com/catalogue/page-44.html\\n\",\n",
        "            \"[info] Fetching page 45: http://books.toscrape.com/catalogue/page-45.html\\n\",\n",
        "            \"[info] Fetching page 46: http://books.toscrape.com/catalogue/page-46.html\\n\",\n",
        "            \"[info] Fetching page 47: http://books.toscrape.com/catalogue/page-47.html\\n\",\n",
        "            \"[info] Fetching page 48: http://books.toscrape.com/catalogue/page-48.html\\n\",\n",
        "            \"[info] Fetching page 49: http://books.toscrape.com/catalogue/page-49.html\\n\",\n",
        "            \"[info] Fetching page 50: http://books.toscrape.com/catalogue/page-50.html\\n\",\n",
        "            \"[done] saved 1000 rows to task1_outputs/books.csv\\n\"\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aieJLhZkAY7",
        "outputId": "4033946b-fe05-4b81-8bb5-b13e64f5c342"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'nbformat': 4,\n",
              " 'nbformat_minor': 0,\n",
              " 'metadata': {'colab': {'provenance': []},\n",
              "  'kernelspec': {'name': 'python3', 'display_name': 'Python 3'},\n",
              "  'language_info': {'name': 'python'}},\n",
              " 'cells': [{'cell_type': 'code',\n",
              "   'execution_count': 4,\n",
              "   'metadata': {'id': 'gi9MbYEGm-Rv'},\n",
              "   'outputs': [],\n",
              "   'source': ['\\n',\n",
              "    '\\n',\n",
              "    'import requests\\n',\n",
              "    'from bs4 import BeautifulSoup\\n',\n",
              "    'import time\\n',\n",
              "    'import csv\\n',\n",
              "    'import random\\n',\n",
              "    'from urllib.parse import urljoin\\n',\n",
              "    'import os']},\n",
              "  {'cell_type': 'code',\n",
              "   'source': ['BASE_URL = \"http://books.toscrape.com/\"\\n',\n",
              "    'START_URL = BASE_URL\\n',\n",
              "    'OUTPUT_DIR = \"task1_outputs\"\\n',\n",
              "    'OUTPUT_CSV = os.path.join(OUTPUT_DIR, \"books.csv\")\\n',\n",
              "    'USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\\\\\\n',\n",
              "    '             \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\\n',\n",
              "    'REQUEST_HEADERS = {\"User-Agent\": USER_AGENT}\\n',\n",
              "    'DELAY_MIN = 1.0\\n',\n",
              "    'DELAY_MAX = 2.5\\n',\n",
              "    'MAX_RETRIES = 3\\n',\n",
              "    'TIMEOUT = 10\\n',\n",
              "    'os.makedirs(OUTPUT_DIR, exist_ok=True)'],\n",
              "   'metadata': {'id': 'D_VwcTyNnmpT'},\n",
              "   'execution_count': 8,\n",
              "   'outputs': []},\n",
              "  {'cell_type': 'code',\n",
              "   'source': ['def safe_get(url, session=None, retries=MAX_RETRIES):\\n',\n",
              "    '    s = session or requests.Session()\\n',\n",
              "    '    for attempt in range(1, retries+1):\\n',\n",
              "    '        try:\\n',\n",
              "    '            resp = s.get(url, headers=REQUEST_HEADERS, timeout=TIMEOUT)\\n',\n",
              "    '            resp.raise_for_status()\\n',\n",
              "    '            return resp\\n',\n",
              "    '        except Exception as e:\\n',\n",
              "    '            print(f\"[warn] request failed ({attempt}/{retries}) for {url}: {e}\")\\n',\n",
              "    '            if attempt == retries:\\n',\n",
              "    '                raise\\n',\n",
              "    '            time.sleep(1.5 * attempt)\\n',\n",
              "    '    return None\\n',\n",
              "    '\\n',\n",
              "    'def random_delay():\\n',\n",
              "    '    time.sleep(random.uniform(DELAY_MIN, DELAY_MAX))'],\n",
              "   'metadata': {'id': 'NBl9yOLrnstJ'},\n",
              "   'execution_count': 9,\n",
              "   'outputs': []},\n",
              "  {'cell_type': 'code',\n",
              "   'source': ['def parse_book_card(card):\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    '    title_tag = card.select_one(\"h3 a\")\\n',\n",
              "    '    title = title_tag[\"title\"].strip() if title_tag and title_tag.has_attr(\"title\") else title_tag.get_text(strip=True)\\n',\n",
              "    '\\n',\n",
              "    '    rel_url = title_tag[\"href\"] if title_tag and title_tag.has_attr(\"href\") else \"\"\\n',\n",
              "    '    product_url = urljoin(BASE_URL, rel_url)\\n',\n",
              "    '\\n',\n",
              "    '    price_tag = card.select_one(\".price_color\")\\n',\n",
              "    '    price = price_tag.get_text(strip=True) if price_tag else \"\"\\n',\n",
              "    '\\n',\n",
              "    '    availability = card.select_one(\".availability\").get_text(strip=True) if card.select_one(\".availability\") else \"\"\\n',\n",
              "    '\\n',\n",
              "    '    rating_class = card.select_one(\"p.star-rating\")\\n',\n",
              "    '    rating = \"\"\\n',\n",
              "    '    if rating_class and rating_class.has_attr(\"class\"):\\n',\n",
              "    '        classes = rating_class[\"class\"]\\n',\n",
              "    '\\n',\n",
              "    '        rating = [c for c in classes if c != \"star-rating\"][0] if len(classes) > 1 else \"\"\\n',\n",
              "    '    return {\\n',\n",
              "    '        \"title\": title,\\n',\n",
              "    '        \"product_url\": product_url,\\n',\n",
              "    '        \"price\": price,\\n',\n",
              "    '        \"availability\": availability,\\n',\n",
              "    '        \"rating\": rating\\n',\n",
              "    '    }\\n',\n",
              "    '\\n',\n",
              "    'def scrape_books(start_url=START_URL, max_pages=None):\\n',\n",
              "    '    results = []\\n',\n",
              "    '    next_page_url = start_url\\n',\n",
              "    '    page_count = 0\\n',\n",
              "    '    session = requests.Session()\\n',\n",
              "    '\\n',\n",
              "    '    while next_page_url:\\n',\n",
              "    '        page_count += 1\\n',\n",
              "    '        print(f\"[info] Fetching page {page_count}: {next_page_url}\")\\n',\n",
              "    '        resp = safe_get(next_page_url, session=session)\\n',\n",
              "    '        soup = BeautifulSoup(resp.text, \"html.parser\")\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    '        cards = soup.select(\"article.product_pod\")\\n',\n",
              "    '        for c in cards:\\n',\n",
              "    '            data = parse_book_card(c)\\n',\n",
              "    '            results.append(data)\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    '        next_btn = soup.select_one(\"li.next a\")\\n',\n",
              "    '        if next_btn and next_btn.has_attr(\"href\"):\\n',\n",
              "    '            rel_next = next_btn[\"href\"]\\n',\n",
              "    '            next_page_url = urljoin(next_page_url, rel_next)\\n',\n",
              "    '        else:\\n',\n",
              "    '            next_page_url = None\\n',\n",
              "    '\\n',\n",
              "    '        page_count += 0\\n',\n",
              "    '        random_delay()\\n',\n",
              "    '\\n',\n",
              "    '        if max_pages and page_count >= max_pages:\\n',\n",
              "    '            print(\"[info] Reached max_pages limit.\")\\n',\n",
              "    '            break\\n',\n",
              "    '\\n',\n",
              "    '    return results\\n'],\n",
              "   'metadata': {'id': 'U13uO6gxn6wR'},\n",
              "   'execution_count': 10,\n",
              "   'outputs': []},\n",
              "  {'cell_type': 'code',\n",
              "   'source': ['if __name__ == \"__main__\":\\n',\n",
              "    '    print(\"[start] scraping started\")\\n',\n",
              "    '    try:\\n',\n",
              "    '        data = scrape_books(max_pages=None)  # set an int to limit pages\\n',\n",
              "    '        # Save to CSV\\n',\n",
              "    '        fieldnames = [\"title\", \"product_url\", \"price\", \"availability\", \"rating\"]\\n',\n",
              "    '        with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\\n',\n",
              "    '            writer = csv.DictWriter(f, fieldnames=fieldnames)\\n',\n",
              "    '            writer.writeheader()\\n',\n",
              "    '            for row in data:\\n',\n",
              "    '                writer.writerow(row)\\n',\n",
              "    '        print(f\"[done] saved {len(data)} rows to {OUTPUT_CSV}\")\\n',\n",
              "    '    except Exception as e:\\n',\n",
              "    '        print(f\"[error] scraping aborted: {e}\")\\n'],\n",
              "   'metadata': {'colab': {'base_uri': 'https://localhost:8080/'},\n",
              "    'id': 'zBmf1tZYoZFb',\n",
              "    'outputId': '043a4def-8fd5-4568-8346-06594494b434'},\n",
              "   'execution_count': 11,\n",
              "   'outputs': [{'output_type': 'stream',\n",
              "     'name': 'stdout',\n",
              "     'text': ['[start] scraping started\\n',\n",
              "      '[info] Fetching page 1: http://books.toscrape.com/\\n',\n",
              "      '[info] Fetching page 2: http://books.toscrape.com/catalogue/page-2.html\\n',\n",
              "      '[info] Fetching page 3: http://books.toscrape.com/catalogue/page-3.html\\n',\n",
              "      '[info] Fetching page 4: http://books.toscrape.com/catalogue/page-4.html\\n',\n",
              "      '[info] Fetching page 5: http://books.toscrape.com/catalogue/page-5.html\\n',\n",
              "      '[info] Fetching page 6: http://books.toscrape.com/catalogue/page-6.html\\n',\n",
              "      '[info] Fetching page 7: http://books.toscrape.com/catalogue/page-7.html\\n',\n",
              "      '[info] Fetching page 8: http://books.toscrape.com/catalogue/page-8.html\\n',\n",
              "      '[info] Fetching page 9: http://books.toscrape.com/catalogue/page-9.html\\n',\n",
              "      '[info] Fetching page 10: http://books.toscrape.com/catalogue/page-10.html\\n',\n",
              "      '[info] Fetching page 11: http://books.toscrape.com/catalogue/page-11.html\\n',\n",
              "      '[info] Fetching page 12: http://books.toscrape.com/catalogue/page-12.html\\n',\n",
              "      '[info] Fetching page 13: http://books.toscrape.com/catalogue/page-13.html\\n',\n",
              "      '[info] Fetching page 14: http://books.toscrape.com/catalogue/page-14.html\\n',\n",
              "      '[info] Fetching page 15: http://books.toscrape.com/catalogue/page-15.html\\n',\n",
              "      '[info] Fetching page 16: http://books.toscrape.com/catalogue/page-16.html\\n',\n",
              "      '[info] Fetching page 17: http://books.toscrape.com/catalogue/page-17.html\\n',\n",
              "      '[info] Fetching page 18: http://books.toscrape.com/catalogue/page-18.html\\n',\n",
              "      '[info] Fetching page 19: http://books.toscrape.com/catalogue/page-19.html\\n',\n",
              "      '[info] Fetching page 20: http://books.toscrape.com/catalogue/page-20.html\\n',\n",
              "      '[info] Fetching page 21: http://books.toscrape.com/catalogue/page-21.html\\n',\n",
              "      '[info] Fetching page 22: http://books.toscrape.com/catalogue/page-22.html\\n',\n",
              "      '[info] Fetching page 23: http://books.toscrape.com/catalogue/page-23.html\\n',\n",
              "      '[info] Fetching page 24: http://books.toscrape.com/catalogue/page-24.html\\n',\n",
              "      '[info] Fetching page 25: http://books.toscrape.com/catalogue/page-25.html\\n',\n",
              "      '[info] Fetching page 26: http://books.toscrape.com/catalogue/page-26.html\\n',\n",
              "      '[info] Fetching page 27: http://books.toscrape.com/catalogue/page-27.html\\n',\n",
              "      '[info] Fetching page 28: http://books.toscrape.com/catalogue/page-28.html\\n',\n",
              "      '[info] Fetching page 29: http://books.toscrape.com/catalogue/page-29.html\\n',\n",
              "      '[info] Fetching page 30: http://books.toscrape.com/catalogue/page-30.html\\n',\n",
              "      '[info] Fetching page 31: http://books.toscrape.com/catalogue/page-31.html\\n',\n",
              "      '[info] Fetching page 32: http://books.toscrape.com/catalogue/page-32.html\\n',\n",
              "      '[info] Fetching page 33: http://books.toscrape.com/catalogue/page-33.html\\n',\n",
              "      '[info] Fetching page 34: http://books.toscrape.com/catalogue/page-34.html\\n',\n",
              "      '[info] Fetching page 35: http://books.toscrape.com/catalogue/page-35.html\\n',\n",
              "      '[info] Fetching page 36: http://books.toscrape.com/catalogue/page-36.html\\n',\n",
              "      '[info] Fetching page 37: http://books.toscrape.com/catalogue/page-37.html\\n',\n",
              "      '[info] Fetching page 38: http://books.toscrape.com/catalogue/page-38.html\\n',\n",
              "      '[info] Fetching page 39: http://books.toscrape.com/catalogue/page-39.html\\n',\n",
              "      '[info] Fetching page 40: http://books.toscrape.com/catalogue/page-40.html\\n',\n",
              "      '[info] Fetching page 41: http://books.toscrape.com/catalogue/page-41.html\\n',\n",
              "      '[info] Fetching page 42: http://books.toscrape.com/catalogue/page-42.html\\n',\n",
              "      '[info] Fetching page 43: http://books.toscrape.com/catalogue/page-43.html\\n',\n",
              "      '[info] Fetching page 44: http://books.toscrape.com/catalogue/page-44.html\\n',\n",
              "      '[info] Fetching page 45: http://books.toscrape.com/catalogue/page-45.html\\n',\n",
              "      '[info] Fetching page 46: http://books.toscrape.com/catalogue/page-46.html\\n',\n",
              "      '[info] Fetching page 47: http://books.toscrape.com/catalogue/page-47.html\\n',\n",
              "      '[info] Fetching page 48: http://books.toscrape.com/catalogue/page-48.html\\n',\n",
              "      '[info] Fetching page 49: http://books.toscrape.com/catalogue/page-49.html\\n',\n",
              "      '[info] Fetching page 50: http://books.toscrape.com/catalogue/page-50.html\\n',\n",
              "      '[done] saved 1000 rows to task1_outputs/books.csv\\n']}]}]}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}